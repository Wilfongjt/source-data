{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.p3_ProcessLogger import ProcessLogger\n",
    "cell_log = ProcessLogger() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Project: Adopt a Drain\n",
    " * Author: James Wilfong, wilfongjt@gmail.com\n",
    " \n",
    "## Basics\n",
    "* data processed in a local clone of source-data \n",
    "* intermediate files are put into source-data repo\n",
    "* the final data.world data set name is the same as the raw-data file name\n",
    "* the source-data repo folders /raw-data, /clean-data, /notebook are updated during the process\n",
    "\n",
    "## Raw-data Process\n",
    "* input: raw-data/ \n",
    "* use python via jupyter notebook to manipulate data into usable file\n",
    "* update results to github\n",
    "* output: clean-data/\n",
    "\n",
    "## GIT Process\n",
    "* input: clean-data/\n",
    "* process: add, commit, push files from raw-data/, clean-data/, notebook/ folders\n",
    "* output: GitHub source-data repo\n",
    "\n",
    "## Data.World Process\n",
    "* input: GitHub source-data/clean-data/\n",
    "* process: transfer github clean-data/ to data.world\n",
    "* output: data.world\n",
    "\n",
    "## Table of Contents\n",
    "* [Introduction](#intro)\n",
    "\n",
    "* [Data Wrangling](#wrangling_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Jupyter Notebook\n",
    "Launch Jupyter Notebook from scripts/app-name folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "* why adopt a drain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prerequisites'></a>\n",
    "## Prerequisites\n",
    "* create [Github](#github) repository to hold raw data\n",
    "* create [Data World](#data-world) account\n",
    "* [Notebook Config](#notebook-config)\n",
    "* [Environment Variable Setup](#env-setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data-world'></a>\n",
    "## Dataworld\n",
    "* Set up an account\n",
    "* DW_AUTH_TOKEN value comes from your [data.world](https://data.world/) account-settings-advanced-Admin.\n",
    "* Application data is stored in data.world\n",
    "* A Data.world dataset is mostly read-only\n",
    "* A Data.world is updated via file replacement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='github'></a>\n",
    "## Github\n",
    "\n",
    "* raw-data is loaded from the remote source-data repo on Github\n",
    "* raw-data is stored in the /raw-data folder of the source-data repo\n",
    "* raw-data is pushed to the remote source-data repo before running this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='env-setup'></a>\n",
    "## Environment Variable Setup\n",
    "\n",
    "* Create a file .env and put in the /scripts/<app-name>/ folder\n",
    "* .env does not get included in the github repository. Exclude .env from github in the .gitignore file\n",
    "* Add environment variables to .env file\n",
    "    * GH_URL=https://raw.githubusercontent.com/Wilfongjt/source-data/master/raw-data/\n",
    "    * DW_DB_URL=https://api.data.world/v0/datasets/wilfongjt/\n",
    "    * DW_USER=your-data-world-user-name\n",
    "    * DW_AUTH_TOKEN=dataworld-adm-token\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Load Packages\n",
       "* Load environment variables\n",
       "* Import third party packages\n",
       "* Import custom packages"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_log.clear()\n",
    "cell_log.collect('## Load Packages')\n",
    "# import dotenv\n",
    "cell_log.collect('* Load environment variables')\n",
    "from settings import *\n",
    "cell_log.collect('* Import third party packages')\n",
    "# from exceptions import ApiException\n",
    "from datadotworld.client import _swagger\n",
    "from datadotworld.client.api import RestApiError\n",
    "import datadotworld as dw\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pprint\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv # read and write csv files\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import Markdown\n",
    "from pprint import pprint\n",
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# convenience functions -- cleaning\n",
    "cell_log.collect('* Import custom packages')\n",
    "from lib.p3_CellCounts import CellCounts\n",
    "import lib.p3_clean as clean\n",
    "from lib.p3_configuration import get_configuration\n",
    "import lib.p3_explore as explore\n",
    "import lib.p3_gather as gather # gathering functions\n",
    "import lib.p3_helper_functions as helper\n",
    "import lib.p3_map as maps\n",
    "\n",
    "Markdown('''{}'''.format(cell_log.getMarkdown()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Helper Functions \n",
       "\n",
       "* get_app_name( script_folder_name )\n",
       "* get_repo_folder( script_folder_name )\n",
       "* get_raw_data_folder( script_folder_name )\n",
       "* get_clean_data_folder( script_folder_name )"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_log.clear()\n",
    "cell_log.collect('## Helper Functions ')\n",
    "cell_log.collect('')\n",
    "cell_log.collect('* get_app_name( script_folder_name )')\n",
    "def get_app_name(scripts_path):\n",
    "    rc = ''\n",
    "    pth = scripts_path.split('/')\n",
    "    rc = pth[len(pth)-1]\n",
    "    return rc \n",
    "cell_log.collect('* get_repo_folder( script_folder_name )')\n",
    "def get_repo_folder(scripts_path):\n",
    "    rc = ''\n",
    "    rc = scripts_path.replace('/' + get_app_name(scripts_path), '').replace('/scripts','')\n",
    "    return rc\n",
    "cell_log.collect('* get_raw_data_folder( script_folder_name )')\n",
    "def get_raw_data_folder(scripts_path):\n",
    "    return get_repo_folder(scripts_path) + '/raw-data/' + get_app_name(scripts_path)\n",
    "cell_log.collect('* get_clean_data_folder( script_folder_name )')    \n",
    "def get_clean_data_folder(scripts_path):\n",
    "    rc = get_repo_folder(scripts_path) + '/clean-data/' + get_app_name(scripts_path)\n",
    "    if not os.path.exists(rc):\n",
    "        os.makedirs(rc)\n",
    "    return rc\n",
    "\n",
    "Markdown('''{}'''.format(cell_log.getMarkdown()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/jameswilfong/anaconda/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: datadotworld[pandas] in /Users/jameswilfong/anaconda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: urllib3<2.0a,>=1.15 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: certifi>=2017.04.17 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: datapackage<1.0a,>=0.8.8 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: flake8<3.4.1a,>=2.6.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: configparser<4.0a,>=3.5.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: click<7.0a,>=6.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: jsontableschema<1.0a,>=0.10.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: requests<3.0a,>=2.0.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: six<2.0a,>=1.5.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: tabulator<=1.4.1 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: python-dateutil<3.0a,>=2.6.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: pandas<1.0a; extra == \"pandas\" in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: unicodecsv<1.0a,>=0.14 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datapackage<1.0a,>=0.8.8->datadotworld[pandas])\n",
      "Requirement already satisfied: jsonschema<3.0a,>=2.5 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datapackage<1.0a,>=0.8.8->datadotworld[pandas])\n",
      "Requirement already satisfied: pycodestyle<2.4.0,>=2.0.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from flake8<3.4.1a,>=2.6.0->datadotworld[pandas])\n",
      "Requirement already satisfied: pyflakes<1.6.0,>=1.5.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from flake8<3.4.1a,>=2.6.0->datadotworld[pandas])\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from flake8<3.4.1a,>=2.6.0->datadotworld[pandas])\n",
      "Requirement already satisfied: isodate<1.0,>=0.5.4 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from jsontableschema<1.0a,>=0.10.0->datadotworld[pandas])\n",
      "Requirement already satisfied: future<1.0,>=0.15 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from jsontableschema<1.0a,>=0.10.0->datadotworld[pandas])\n",
      "Requirement already satisfied: rfc3986<1.0,>=0.4 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from jsontableschema<1.0a,>=0.10.0->datadotworld[pandas])\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from requests<3.0a,>=2.0.0->datadotworld[pandas])\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from requests<3.0a,>=2.0.0->datadotworld[pandas])\n",
      "Requirement already satisfied: linear-tsv<2.0,>=1.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from tabulator<=1.4.1->datadotworld[pandas])\n",
      "Requirement already satisfied: cchardet<2.0,>=1.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from tabulator<=1.4.1->datadotworld[pandas])\n",
      "Requirement already satisfied: sqlalchemy<2.0,>=1.1 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from tabulator<=1.4.1->datadotworld[pandas])\n",
      "Requirement already satisfied: jsonlines<2.0,>=1.1 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from tabulator<=1.4.1->datadotworld[pandas])\n",
      "Requirement already satisfied: ijson<3.0,>=2.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from tabulator<=1.4.1->datadotworld[pandas])\n",
      "Requirement already satisfied: xlrd<2.0,>=1.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from tabulator<=1.4.1->datadotworld[pandas])\n",
      "Requirement already satisfied: openpyxl<3.0,>=2.4 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from tabulator<=1.4.1->datadotworld[pandas])\n",
      "Requirement already satisfied: pytz>=2011k in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from pandas<1.0a; extra == \"pandas\"->datadotworld[pandas])\n",
      "Requirement already satisfied: numpy>=1.7.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from pandas<1.0a; extra == \"pandas\"->datadotworld[pandas])\n",
      "Requirement already satisfied: jdcal in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from openpyxl<3.0,>=2.4->tabulator<=1.4.1->datadotworld[pandas])\n",
      "Requirement already satisfied: et_xmlfile in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from openpyxl<3.0,>=2.4->tabulator<=1.4.1->datadotworld[pandas])\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<a id='notebook-config'></a>\n",
       "## Notebook Config\n",
       "* python-dotenv\n",
       "* datadotworld"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env\n",
    "\n",
    "cell_log.clear()\n",
    "cell_log.collect(\"<a id='notebook-config'></a>\")\n",
    "cell_log.collect(\"## Notebook Config\")\n",
    "# ------------ environment variable magic\n",
    "\n",
    "# Install a pip packages in the current Jupyter kernel\n",
    "# ------------ Python-dotenv\n",
    "cell_log.collect(\"* python-dotenv\")\n",
    "import sys\n",
    "!{sys.executable} -m pip install python-dotenv\n",
    "# ------------ data.world API \n",
    "cell_log.collect(\"* datadotworld\")\n",
    "!{sys.executable} -m pip install datadotworld[pandas]\n",
    "\n",
    "Markdown('''{}'''.format(cell_log.getMarkdown()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "## Prepare Data\n",
    "* download github repo with data\n",
    "* put new file in raw-data/\n",
    "* make copy of this jupyter notebook \n",
    "* configure to transform raw-data/ into clean-data/\n",
    "* put clean data into clean/ folder\n",
    "* push final changes to github\n",
    "## Load Data\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wrangling_steps'></a>\n",
    "# Data Wrangling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wrangle-process'></a>\n",
    "## Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"download\"></a>\n",
    "# Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "table_name: * gr_drains\n",
       "* ext: csv\n",
       "* table_name: GRB Storm Drains\n",
       "* desc: Storm Drains of the Grand River Basin, Michigan\n",
       "* MODE: prod\n",
       "* GH_URL: https://raw.githubusercontent.com/Wilfongjt/source-data/refresh-data/clean-data/\n",
       "* DW_DB_URL: https://api.data.world/v0/datasets/wilfongjt/\n",
       "* LOCAL_SCRIPT_FOLDER: /Users/jameswilfong/Documents/Github/Wilfongjt/source-data/scripts/adopt-a-drain\n",
       "* LOCAL_APP_NAME: adopt-a-drain\n",
       "* LOCAL_REPO_FOLDER: /Users/jameswilfong/Documents/Github/Wilfongjt/source-data\n",
       "* LOCAL_REPO_BRANCH: refresh-data\n",
       "* LOCAL_RAW_FOLDER: /Users/jameswilfong/Documents/Github/Wilfongjt/source-data/raw-data/adopt-a-drain\n",
       "* LOCAL_CLEAN_FOLDER: /Users/jameswilfong/Documents/Github/Wilfongjt/source-data/clean-data/adopt-a-drain"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_log.clear()\n",
    "# dev stops github and data.world updates\n",
    "# prod allows github and data.world updates\n",
    "MODE='prod' # dev, prod\n",
    "'''\n",
    "------------- configure raw-data\n",
    "'''\n",
    "\n",
    "table_name = '* gr_drains'\n",
    "cell_log.collect('table_name: {}'.format(table_name))\n",
    "\n",
    "\n",
    "ext = 'csv'\n",
    "cell_log.collect('* ext: {}'.format(ext))\n",
    "\n",
    "\n",
    "title_name = 'GRB Storm Drains'\n",
    "cell_log.collect('* table_name: {}'.format(title_name))\n",
    "\n",
    "\n",
    "desc = 'Storm Drains of the Grand River Basin, Michigan'\n",
    "cell_log.collect('* desc: {}'.format(desc))\n",
    "\n",
    "\n",
    "'''\n",
    "-------------- setup GitHub\n",
    "'''\n",
    "LOCAL_REPO_BRANCH = 'refresh-data'\n",
    "# cell_log.collect('* repo_branch: {}'.format(repo_branch))\n",
    "\n",
    "\n",
    "'''\n",
    "------------- setup constants to GitHub folders\n",
    "'''\n",
    "\n",
    "LOCAL_SCRIPT_FOLDER = os.getcwd()\n",
    "LOCAL_APP_NAME = get_app_name(LOCAL_SCRIPT_FOLDER)\n",
    "LOCAL_REPO_FOLDER =  get_repo_folder(LOCAL_SCRIPT_FOLDER)  \n",
    "LOCAL_RAW_FOLDER = get_raw_data_folder(LOCAL_SCRIPT_FOLDER) \n",
    "LOCAL_CLEAN_FOLDER = get_clean_data_folder(LOCAL_SCRIPT_FOLDER)\n",
    "\n",
    "# GH_URL=\"https://raw.githubusercontent.com/Wilfongjt/source-data/refresh-data/clean-data/\"\n",
    "# DW_DB_URL=\"https://api.data.world/v0/datasets/wilfongjt/\"\n",
    "\n",
    "cell_log.collect('* MODE: {}'.format(MODE))\n",
    "\n",
    "cell_log.collect('* GH_URL: {}'.format(GH_URL))\n",
    "cell_log.collect('* DW_DB_URL: {}'.format(DW_DB_URL))\n",
    "cell_log.collect('* LOCAL_SCRIPT_FOLDER: {}'.format(LOCAL_SCRIPT_FOLDER))\n",
    "cell_log.collect('* LOCAL_APP_NAME: {}'.format(LOCAL_APP_NAME))\n",
    "cell_log.collect('* LOCAL_REPO_FOLDER: {}'.format(LOCAL_REPO_FOLDER))\n",
    "cell_log.collect('* LOCAL_REPO_BRANCH: {}'.format(LOCAL_REPO_BRANCH))\n",
    "\n",
    "\n",
    "cell_log.collect('* LOCAL_RAW_FOLDER: {}'.format(LOCAL_RAW_FOLDER))\n",
    "cell_log.collect('* LOCAL_CLEAN_FOLDER: {}'.format(LOCAL_CLEAN_FOLDER))\n",
    "\n",
    "Markdown('''{}'''.format(cell_log.getMarkdown()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'app_name': 'adopt-a-drain',\n",
      " 'dw_dataset_id': 'wilfongjt/grb-storm-drains',\n",
      " 'dw_desc': 'Storm Drains of the Grand River Basin, Michigan',\n",
      " 'dw_table': '* gr_drains',\n",
      " 'dw_title': 'GRB Storm Drains',\n",
      " 'dw_url': 'https://api.data.world/v0/datasets/wilfongjt/* gr_drains.csv',\n",
      " 'files': {'* gr_drains.csv': {'url': 'https://raw.githubusercontent.com/Wilfongjt/source-data/refresh-data/clean-data/* '\n",
      "                                      'gr_drains.csv'}},\n",
      " 'gh_url': 'https://raw.githubusercontent.com/Wilfongjt/source-data/refresh-data/clean-data/* '\n",
      "           'gr_drains',\n",
      " 'license': 'Public Domain',\n",
      " 'local_clean': '/Users/jameswilfong/Documents/Github/Wilfongjt/source-data/clean-data/adopt-a-drain/* '\n",
      "                'gr_drains.csv',\n",
      " 'local_raw': '/Users/jameswilfong/Documents/Github/Wilfongjt/source-data/raw-data/adopt-a-drain/* '\n",
      "              'gr_drains.csv',\n",
      " 'owner_id': 'wilfongjt',\n",
      " 'visibility': 'OPEN'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dw_dataset_id = DW_USER + \"/\" + title_name.lower().replace('_','-').replace(' ','-')\n",
    "gh_csv_name = table_name\n",
    "gh_csv_name_ext = gh_csv_name + '.' + ext\n",
    "gh_csv_path_name = GH_URL + gh_csv_name_ext\n",
    "\n",
    "'''\n",
    "------------- configure source csv\n",
    "'''\n",
    "\n",
    "# tbl = getTableDef(title_name, desc, table_name)\n",
    "tbl = { \"owner_id\": DW_USER, \n",
    "        \"app_name\": LOCAL_APP_NAME,\n",
    "             \"dw_title\": title_name, \n",
    "             \"dw_desc\": desc,\n",
    "             \"dw_table\": table_name,\n",
    "             \"dw_dataset_id\": dw_dataset_id,\n",
    "             \"dw_url\": DW_DB_URL + table_name + '.' + ext,\n",
    "             \"gh_url\": GH_URL + table_name, \n",
    "             \"visibility\": \"OPEN\", \n",
    "             \"license\": \"Public Domain\",\n",
    "             \"files\": {table_name + '.' + 'csv': {\"url\": gh_csv_path_name }},\n",
    "             \"local_raw\": LOCAL_RAW_FOLDER + '/' + gh_csv_name_ext,\n",
    "             \"local_clean\": LOCAL_CLEAN_FOLDER + '/' + gh_csv_name_ext,\n",
    "           }\n",
    "'''\n",
    "------------- configure outliers\n",
    "'''\n",
    "_outliers = {\n",
    "  'outliers': [\n",
    "    {'column':'dr_facility_id',\n",
    "     'range':(1, 50000000),\n",
    "     'reason':'ignore {} outliers (1 <= dr_facility_id or => 50000000).',\n",
    "     'count': 0\n",
    "    }, \n",
    "    {'column':'dr_lon',\n",
    "     'range':(-90.0, -80.0),\n",
    "     'reason':'Remove {} observations too far west or east.',\n",
    "     'count': 0\n",
    "    },  \n",
    "    {'column':'dr_lat',\n",
    "     'range':(40.0, 50.0),\n",
    "     'reason':'Remove {} observations too far north or south.',\n",
    "     'count': 0\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "pprint( tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSourceData(tblDef):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    return pd.read_csv(tblDef[\"local_raw\"])\n",
    "'''        \n",
    "def getTableDef(title, desc ,table_name, ext='csv'):\n",
    "    dw_title = title\n",
    "    dw_table = table_name.replace(' ','_').replace('-','_')\n",
    "    dw_dataset_id = DW_USER + \"/\" + title.lower().replace('_','-').replace(' ','-')\n",
    "    dw_desc = desc\n",
    "    gh_csv_name = table_name\n",
    "    gh_csv_name_ext = gh_csv_name + '.' + ext\n",
    "    gh_csv_path_name = GH_URL + gh_csv_name_ext\n",
    "    \n",
    "    return { \"owner_id\": DW_USER, \n",
    "             \"dw_title\": dw_title, \n",
    "             \"dw_desc\": dw_desc,\n",
    "             \"dw_table\": dw_table,\n",
    "             \"dw_dataset_id\": dw_dataset_id,\n",
    "             \"dw_url\": DW_DB_URL + table_name + '.' + ext,\n",
    "             \"gh_url\": GH_URL + table_name, \n",
    "             \"visibility\": \"OPEN\", \n",
    "             \"license\": \"Public Domain\",\n",
    "             \"files\": {table_name + '.' + 'csv': {\"url\": gh_csv_path_name }},\n",
    "             \"local_raw\": LOCAL_RAW_FOLDER + gh_csv_name_ext,\n",
    "             \"local_clean\": LOCAL_CLEAN_FOLDER + gh_csv_name_ext,\n",
    "           }\n",
    "'''\n",
    "def loadDataWorld(tbl_def):\n",
    "    '''\n",
    "        Takes a csv file and imports it into dataworld\n",
    "        tbl_def is { \"owner_id\": DW_USER, \n",
    "                     \"dw_title\": table_name, \n",
    "                     \"gh_url\": GH_URL + table_name, \n",
    "                     \"visibility\": \"OPEN\", \n",
    "                     \"license\": \"Public Domain\",\n",
    "                     \"files\": {table_name + '.csv': {\"url\": GH_URL + table_name + '.csv'}},\n",
    "                     \"dw_url\": DW_DB_URL + table_name + '.csv' \n",
    "                    }\n",
    "                    \n",
    "    '''\n",
    "    # api_client.create_dataset(\n",
    "    dw.api_client().create_dataset(    \n",
    "        owner_id=tbl_def[\"owner_id\"], \n",
    "        title=tbl_def[\"dw_title\"], \n",
    "        description=tbl_def[\"dw_desc\"],\n",
    "        visibility=tbl_def[\"visibility\"],\n",
    "        license=tbl_def['license'],\n",
    "        files=tbl_def[\"files\"]\n",
    "    )\n",
    "    \n",
    "    \n",
    "def deleteDataWorld(tbl_def):\n",
    "    '''\n",
    "    Removes table from data.world\n",
    "    tbl_def is { \"owner_id\": DW_USER, \n",
    "                     \"dw_title\": table_name, \n",
    "                     \"gh_url\": GH_URL + table_name, \n",
    "                     \"visibility\": \"OPEN\", \n",
    "                     \"license\": \"Public Domain\",\n",
    "                     \"files\": {table_name + '.csv': {\"url\": GH_URL + table_name + '.csv'}},\n",
    "                     \"dw_url\": DW_DB_URL + table_name + '.csv',\n",
    "                     \"dataset_id\": DW_USER + \"/\" + table_name\n",
    "                    }\n",
    "    '''\n",
    "    \n",
    "    dw.api_client().delete_dataset(       \n",
    "        tbl_def[\"dw_dataset_id\"]\n",
    "    )\n",
    "        \n",
    "def updateDataWorld():\n",
    "    '''\n",
    "        Takes a csv file and imports it into dataworld\n",
    "        tbl_def is { \"owner_id\": DW_USER, \n",
    "                     \"dw_title\": table_name, \n",
    "                     \"gh_url\": GH_URL + table_name, \n",
    "                     \"visibility\": \"OPEN\", \n",
    "                     \"license\": \"Public Domain\",\n",
    "                     \"files\": {table_name + '.csv': {\"url\": GH_URL + table_name + '.csv'}},\n",
    "                     \"dw_url\": DW_DB_URL + table_name + '.csv',\n",
    "                     \"dw_dataset_key\": DW_USER + \"/\" + table_name\n",
    "                    }\n",
    "                    \n",
    "    '''\n",
    "    # owner_id=tbl_def[\"owner_id\"], \n",
    "    dw.api_client().update_dataset(    \n",
    "        \n",
    "        title=tbl_def[\"dw_title\"], \n",
    "        visibility=tbl_def[\"visibility\"],\n",
    "        license=tbl_def['license'],\n",
    "        files=tbl_def[\"files\"]\n",
    "    )    \n",
    "    \n",
    "    \n",
    "# def renameColumns(df,):    \n",
    "#    df = df.rename(columns=clean_column_names)    \n",
    "\n",
    "def setDuplicates(default_value, dup_list):\n",
    "    return lambda default_value : a * n\n",
    "\n",
    "    \n",
    "def getCodes(df_source):\n",
    "    muni_list = df_source.dr_muni_id.unique()\n",
    "    inc = 10000000\n",
    "    print(muni_list)\n",
    "    muni_codes = {}\n",
    "    cnt = 0\n",
    "    for x in muni_list:    \n",
    "        muni_codes[x] = inc * cnt\n",
    "        cnt += 1\n",
    "            \n",
    "        \n",
    "    return muni_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wrangle(tbl, _outliers, df_source, MODE ,cell_log):\n",
    "    cell_log.clear()\n",
    "\n",
    "\n",
    "    cell_log.collect(\"# CSV Process\")\n",
    "    '''\n",
    "    --------------------------------- input\n",
    "    '''\n",
    "    # for tbl in tables:\n",
    "    #    cell_log.collect(\"* input:  {}\".format( tbl[\"local_raw\"]))\n",
    "    cell_log.collect(\"* input:  {}\".format( tbl[\"local_raw\"]))\n",
    "    '''\n",
    "    --------------------------------- load data\n",
    "    ''' \n",
    "    # tbl = tables[0]\n",
    "    # df_source = pd.read_csv(tbl[\"local_raw\"])\n",
    "    # print(tbl)\n",
    "    # df_source = getSourceData(tbl)\n",
    "    cell_log.collect(\"* input: {} observations\".format(len(df_source)))\n",
    "    cell_log.collect(\"* input: columns {}\".format(df_source.columns.values))\n",
    "\n",
    "    '''\n",
    "    --------------------------------- clean column names\n",
    "    '''\n",
    "    cell_log.collect('* format: Apply a style of lowercase and underscores to column names.')##############################\n",
    "    df_source = clean.clean_column_names(df_source)\n",
    "\n",
    "    '''\n",
    "    --------------------------------- rename columns\n",
    "    '''\n",
    "    # df_source['lon'] = df_source['trk_crnt_x_cord']\n",
    "    # df_source['lat'] = df_source['trk_crnt_y_cord']\n",
    "    df_source = df_source.rename(columns={\n",
    "        \"subtype\": \"dr_subtype\",\n",
    "        \"drain__owner\": \"dr_owner\",\n",
    "        \"local__id\": \"dr_local_id\",\n",
    "        \"facilityid\": \"dr_facility_id\",\n",
    "        \"drain__jurisdiction\": \"dr_jurisdiction\",\n",
    "        \"subwatershed\": \"dr_subwatershed\",\n",
    "        \"point__x\":\"dr_lon\", \n",
    "        \"point__y\":\"dr_lat\"})\n",
    "\n",
    "    # print('info: ',df_source.info())\n",
    "    '''\n",
    "    --------------------------------- change empty values\n",
    "    '''\n",
    "    # change '', ' ', None, and NaN to -1\n",
    "    # def_facility_id = _outliers['outliers'][0]['range'][0] - 1\n",
    "    # def_facility_id_dup =  def_facility_id - 1\n",
    "    # cell_log.collect(\"* clean: mark bad dr_facility_id values ('',' ',None,NaN') with {}\".format(def_facility_id))\n",
    "\n",
    "    ## ------------------------------ DROP empty Facility id\n",
    "    # mark all empties with same value\n",
    "    df_source['dr_facility_id'] = df_source['dr_facility_id'].apply(lambda x:  np.nan if x != x or x == '' or x == ' ' or x == None else x)\n",
    "    scnt = len(df_source)\n",
    "    df_source = df_source.dropna(subset=['dr_facility_id', 'soure__id','dr_lon', 'dr_lat'])\n",
    "    ecnt = len(df_source)\n",
    "    cell_log.collect(\"* clean: dropped {} observations with empty dr_facility_id, soure___id, dr_lon, or dr_lat\".format(scnt - ecnt))\n",
    "\n",
    "\n",
    "    '''\n",
    "    --------------------------------- change column types\n",
    "    '''\n",
    "    cell_log.collect('* format: convert dr_facility_id column to int64')\n",
    "    df_source['dr_facility_id'] = df_source['dr_facility_id'].astype('int64')\n",
    "\n",
    "    '''\n",
    "    --------------------------------- remove numbers from df_source_id\n",
    "    '''\n",
    "    \n",
    "    df_source['soure__id'] = df_source['soure__id'].apply(lambda x: x.split('_')[0] + '_' if isinstance(x, str) else 'XXX_') \n",
    "    \n",
    "    df_source['dr_asset_no'] = df_source['dr_facility_id']\n",
    "    df_source['dr_type'] = df_source['dr_facility_id'].apply(lambda x: 'Storm Water Inlet Drain')\n",
    "    '''\n",
    "    --------------------------------- create a sync id\n",
    "    '''\n",
    "\n",
    "    df_source['dr_sync_id'] = df_source['soure__id'] + df_source['dr_facility_id'].astype(str)\n",
    "\n",
    "    '''\n",
    "    --------------------------------- drop soure__id\n",
    "    '''\n",
    "    df_source = df_source.drop(['soure__id'], axis=1)\n",
    "\n",
    "    '''\n",
    "    --------------------------------- outliers\n",
    "    '''\n",
    "    df_source = clean.remove_obvious_outliers(_outliers, df_source)\n",
    "    # cell_log.collect('# Outliers')\n",
    "    for r in _outliers['outliers']:##############################\n",
    "        cell_log.collect('* outlier: {}'.format(r['reason']))\n",
    "\n",
    "    '''\n",
    "    --------------------------------- Drop DUPLICATES\n",
    "    '''\n",
    "    scnt = len(df_source)\n",
    "    df_source = df_source.drop_duplicates('dr_facility_id',keep=False)\n",
    "    ecnt = len(df_source)\n",
    "    cell_log.collect('* duplicates: dropped {} duplicate facility ids'.format(scnt - ecnt))\n",
    "\n",
    "\n",
    "    '''\n",
    "    --------------------------------- save csv \n",
    "    '''\n",
    "    # cell_log.collect('# Output')\n",
    "    # assume new file and remove old one\n",
    "    if os.path.isfile(tbl[\"local_clean\"]):\n",
    "        os.remove(tbl['local_clean'])\n",
    "        cell_log.collect('* system: remove {} '.format(tbl['local_clean']))\n",
    "\n",
    "    cell_log.collect(\"* inter-output: columns {}\".format(df_source.columns.values))\n",
    "    cell_log.collect('* inter-output: {} obs to {}'.format(len(df_source) , tbl[\"local_clean\"]))\n",
    "\n",
    "\n",
    "    df_source.to_csv(tbl[\"local_clean\"], index=False)\n",
    "\n",
    "\n",
    "    # if MODE == 'dev':\n",
    "    #    print('info: ',df_source.info())\n",
    "    #    print('head: ',df_source.head())  \n",
    "    #    print('outliers: ', _outliers)\n",
    "\n",
    "\n",
    "    if MODE == 'prod':    \n",
    "        '''\n",
    "        run extra git commands\n",
    "        run import to data.word\n",
    "        '''\n",
    "        '''\n",
    "        --------------------------------- GIT Process \n",
    "        '''\n",
    "        cell_log.collect('')\n",
    "        cell_log.collect('# GIT Process')\n",
    "        '''\n",
    "        --------------------------------- input\n",
    "        '''\n",
    "        cell_log.collect('* input: ' + tbl[\"local_clean\"])\n",
    "        '''\n",
    "        --------------------------------- git add\n",
    "        '''\n",
    "        cell_log.collect('* git add raw-data/ -A')\n",
    "        output = subprocess.check_output([\"git\", \"add\", \"../../raw-data/{}\".format(tbl['app_name']) ,\"-A\"])\n",
    "        cell_log.collect('* git add clean-data/ -A' )\n",
    "        output = subprocess.check_output([\"git\", \"add\", \"../../clean-data/{}\".format(tbl['app_name']) ,\"-A\"])\n",
    "        cell_log.collect('* git add scripts/ -A' )\n",
    "        output = subprocess.check_output([\"git\", \"add\", \"../../scripts/{}\".format(tbl['app_name']),\"-A\"])\n",
    "        cell_log.collect('* git add ../README.md -A' )\n",
    "        output = subprocess.check_output([\"git\", \"add\", \"../../README.md\"])\n",
    "        '''\n",
    "        --------------------------------- git commit\n",
    "        '''\n",
    "        # cell_log.collect('* XXXXXXX git commit -m \"update raw-data {}\"'.format(tbl[\"local_raw\"]) )\n",
    "        # cell_log.collect('* XXXXXXX git commit -m \"update clean-data {}\"'.format(\"local_clean\") )\n",
    "        cell_log.collect('* git commit -m \"update raw-data, clean-data, and scripts\"' )\n",
    "\n",
    "        # output = subprocess.check_output([\"git\", \"commit\", \"-m\", \"'update raw-data, clean-data, and notebook files'\"])\n",
    "\n",
    "        try:\n",
    "            output = subprocess.check_output([\"git\", \"commit\", \"-m\", \"'update raw-data, clean-data, and notebook files'\"])\n",
    "        except subprocess.CalledProcessError as error:\n",
    "            print(error)\n",
    "        except:\n",
    "            cell_log.collect('* unknown error' )\n",
    "        '''\n",
    "        --------------------------------- git push\n",
    "        '''\n",
    "        cell_log.collect('* git push origin ' + repo_branch)\n",
    "        output = subprocess.check_output([\"git\", \"push\", \"origin\", repo_branch])\n",
    "\n",
    "        '''\n",
    "        --------------------------------- Data World Process \n",
    "        '''\n",
    "    else:\n",
    "        '''\n",
    "        --------------------------------- GIT Process \n",
    "        '''\n",
    "        cell_log.collect('')\n",
    "        cell_log.collect('# GIT Process')\n",
    "        '''\n",
    "        --------------------------------- input\n",
    "        '''\n",
    "        cell_log.collect('* input: ' + tbl[\"local_clean\"])\n",
    "        cell_log.collect('* Set [MODE](#download) = \"prod\" to push to GitHub')\n",
    "        cell_log.collect('* output: no output')\n",
    "        \n",
    "    if MODE == 'prod': \n",
    "        cell_log.collect('')\n",
    "        cell_log.collect('# Data.World Process')\n",
    "        cell_log.collect('* input: {}'.format(tbl[\"gh_url\"] + \".csv\") )\n",
    "        cell_log.collect('* load: {} observations'.format(len(df_source)))\n",
    "\n",
    "        try:\n",
    "            deleteDataWorld(tbl)\n",
    "            cell_log.collect('* drop: {}'.format(tbl[\"dw_dataset_id\"]))\n",
    "        except RestApiError as ex:\n",
    "            print(' ' )\n",
    "            # cell_log.collect('* No need to drop {}'.format(tbl[\"dw_dataset_id\"]))\n",
    "\n",
    "        del_delay = 6\n",
    "        time.sleep(del_delay)\n",
    "\n",
    "        try:\n",
    "            cell_log.collect('* delay: {} seconds to delete data'.format(del_delay))\n",
    "            loadDataWorld(tbl)\n",
    "            cell_log.collect('* load: complete')\n",
    "            cell_log.collect('* output: {}'.format(tbl[\"dw_url\"] ))\n",
    "\n",
    "        except RestApiError as ex:\n",
    "            print('RestApiException create: ' + str(ex) )\n",
    "            cell_log.collect('* LOAD FAIL: {}'.format(str(ex)))\n",
    "\n",
    "        return df_source\n",
    "    else:\n",
    "        cell_log.collect('')\n",
    "        cell_log.collect('# Data.World Process')\n",
    "        cell_log.collect('* input: {}'.format(tbl[\"gh_url\"] + \".csv\") )\n",
    "        cell_log.collect('* Set [MODE](#download) = \"prod\" to update data.world')\n",
    "        cell_log.collect('* output: no output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/Users/jameswilfong/Documents/Github/Wilfongjt/source-data/raw-data/adopt-a-drain/* gr_drains.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-1de1985d6c0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSourceData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtbl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_outliers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'''{}'''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-a69e63969fe4>\u001b[0m in \u001b[0;36mgetSourceData\u001b[0;34m(tblDef)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetSourceData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtblDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtblDef\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"local_raw\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m '''        \n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetTableDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdw_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/Users/jameswilfong/Documents/Github/Wilfongjt/source-data/raw-data/adopt-a-drain/* gr_drains.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df_source = getSourceData(tbl)\n",
    "df_source = wrangle(tbl, _outliers, df_source, MODE, cell_log)\n",
    "Markdown('''{}'''.format(cell_log.getMarkdown()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Appendix - Data.World Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping the names straight\n",
    "\n",
    "| CSV Name      | Table Name    | Title          | Dataset ID      | Restful |\n",
    "| :------------ |:------------- | :------------- | :-------------  | :------------- |\n",
    "| xxxx_xx       | xxxx_xx       | Xxxx Xx        | xxxx-xx         |    ?     | \n",
    "| xxxx_xx       | xxxx_xx       | Xxxx_Xx        | xxxxxx          |    ?            |\n",
    "| xxxx_xx       | xxxx_xx       | Xxxx-Xx        | xxxx-xx         |    ?         |\n",
    "| xxxx-xx       | xxxx_xx       | Xxxx Xx        | xxxx-xx         |    ?         |\n",
    "| xxxx-xx       | xxxx_xx       | Xxxx_Xx        | xxxxxx          |    ?         |\n",
    "| xxxx-xx       | xxxx_xx       | Xxxx-Xx        | xxxx-xx         |    ?         |\n",
    "\n",
    "* CSV Name is root of Table name\n",
    "* Title is root of Dataset ID\n",
    "* a space in Title will be automatically converted to hyphen in dataset id\n",
    "* an underscore in Title will be removed in Dataset ID\n",
    "* a hyphen in CSV Name will be replaced with underscore in Table Name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
