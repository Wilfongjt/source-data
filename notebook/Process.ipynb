{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.p3_ProcessLogger import ProcessLogger\n",
    "cell_log = ProcessLogger() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Project: Adopt a Drain\n",
    " * Author: James Wilfong, wilfongjt@gmail.com\n",
    " \n",
    "## Basics\n",
    "* data processed in a local clone of source-data \n",
    "* intermediate files are put into source-data repo\n",
    "* the final data.world data set name is the same as the raw-data file name\n",
    "* the source-data repo folders /raw-data, /clean-data, /notebook are updated during the process\n",
    "\n",
    "## Raw-data Process\n",
    "* input: raw-data/ \n",
    "* use python via jupyter notebook to manipulate data into usable file\n",
    "* update results to github\n",
    "* output: clean-data/\n",
    "\n",
    "## GIT Process\n",
    "* input: clean-data/\n",
    "* process: add, commit, push files from raw-data/, clean-data/, notebook/ folders\n",
    "* output: GitHub source-data repo\n",
    "\n",
    "## Data.World Process\n",
    "* input: GitHub source-data/clean-data/\n",
    "* process: transfer github clean-data/ to data.world\n",
    "* output: data.world\n",
    "\n",
    "## Table of Contents\n",
    "* [Introduction](#intro)\n",
    "\n",
    "* [Data Wrangling](#wrangling_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "* why adopt a drain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prerequisites'></a>\n",
    "## Prerequisites\n",
    "* create [Github](#github) repository to hold raw data\n",
    "* create [Data World](#data-world) account\n",
    "* [Notebook Config](#notebook-config)\n",
    "* [Environment Variable Setup](#env-setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data-world'></a>\n",
    "## Dataworld\n",
    "* Set up an account\n",
    "* DW_AUTH_TOKEN value comes from your [data.world](https://data.world/) account-settings-advanced-Admin.\n",
    "* Application data is stored in data.world\n",
    "* A Data.world dataset is mostly read-only\n",
    "* A Data.world is updated via file replacement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='github'></a>\n",
    "## Github\n",
    "\n",
    "* raw-data is loaded from the remote source-data repo on Github\n",
    "* raw-data is stored in the /raw-data folder of the source-data repo\n",
    "* raw-data is pushed to the remote source-data repo before running this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='env-setup'></a>\n",
    "## Environment Variable Setup\n",
    "* Create a file .env and put in the /notebook folder\n",
    "* .env does not get included in the github repository. Exclude .env from github in the .gitignore file\n",
    "* Add environment variables to .env file\n",
    "    * DW_USER=your-data-world-user-name\n",
    "    * GH_URL=https://raw.githubusercontent.com/Wilfongjt/source-data/master/raw-data/\n",
    "    * DW_DB_URL=https://api.data.world/v0/datasets/wilfongjt/\n",
    "    * DW_DB_RW_TOKEN=dataworld-token\n",
    "    * DW_ADM_TOKEN=dataworld-adm-token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "settings\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Load Packages\n",
       "* Load environment variables\n",
       "* Import third party packages\n",
       "* Import custom packages"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_log.clear()\n",
    "cell_log.collect('## Load Packages')\n",
    "# import dotenv\n",
    "cell_log.collect('* Load environment variables')\n",
    "from settings import *\n",
    "cell_log.collect('* Import third party packages')\n",
    "# from exceptions import ApiException\n",
    "from datadotworld.client import _swagger\n",
    "from datadotworld.client.api import RestApiError\n",
    "import datadotworld as dw\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv # read and write csv files\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import Markdown\n",
    "from pprint import pprint\n",
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# convenience functions -- cleaning\n",
    "cell_log.collect('* Import custom packages')\n",
    "from lib.p3_CellCounts import CellCounts\n",
    "import lib.p3_clean as clean\n",
    "from lib.p3_configuration import get_configuration\n",
    "import lib.p3_explore as explore\n",
    "import lib.p3_gather as gather # gathering functions\n",
    "import lib.p3_helper_functions as helper\n",
    "import lib.p3_map as maps\n",
    "\n",
    "Markdown('''{}'''.format(cell_log.getMarkdown()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/jameswilfong/anaconda/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: datadotworld[pandas] in /Users/jameswilfong/anaconda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: jsontableschema<1.0a,>=0.10.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: datapackage<1.0a,>=0.8.8 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: python-dateutil<3.0a,>=2.6.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: requests<3.0a,>=2.0.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: tabulator<=1.4.1 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: certifi>=2017.04.17 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: flake8<3.4.1a,>=2.6.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: configparser<4.0a,>=3.5.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: urllib3<2.0a,>=1.15 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: click<7.0a,>=6.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: six<2.0a,>=1.5.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: pandas<1.0a; extra == \"pandas\" in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from datadotworld[pandas])\n",
      "Requirement already satisfied: future<1.0,>=0.15 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from jsontableschema<1.0a,>=0.10.0->datadotworld[pandas])\n",
      "Requirement already satisfied: unicodecsv<1.0,>=0.14 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from jsontableschema<1.0a,>=0.10.0->datadotworld[pandas])\n",
      "Requirement already satisfied: isodate<1.0,>=0.5.4 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from jsontableschema<1.0a,>=0.10.0->datadotworld[pandas])\n",
      "Requirement already satisfied: jsonschema<3.0,>=2.5 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from jsontableschema<1.0a,>=0.10.0->datadotworld[pandas])\n",
      "Requirement already satisfied: rfc3986<1.0,>=0.4 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from jsontableschema<1.0a,>=0.10.0->datadotworld[pandas])\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from requests<3.0a,>=2.0.0->datadotworld[pandas])\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from requests<3.0a,>=2.0.0->datadotworld[pandas])\n",
      "Requirement already satisfied: cchardet<2.0,>=1.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from tabulator<=1.4.1->datadotworld[pandas])\n",
      "Requirement already satisfied: openpyxl<3.0,>=2.4 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from tabulator<=1.4.1->datadotworld[pandas])\n",
      "Requirement already satisfied: ijson<3.0,>=2.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from tabulator<=1.4.1->datadotworld[pandas])\n",
      "Requirement already satisfied: sqlalchemy<2.0,>=1.1 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from tabulator<=1.4.1->datadotworld[pandas])\n",
      "Requirement already satisfied: jsonlines<2.0,>=1.1 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from tabulator<=1.4.1->datadotworld[pandas])\n",
      "Requirement already satisfied: xlrd<2.0,>=1.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from tabulator<=1.4.1->datadotworld[pandas])\n",
      "Requirement already satisfied: linear-tsv<2.0,>=1.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from tabulator<=1.4.1->datadotworld[pandas])\n",
      "Requirement already satisfied: pycodestyle<2.4.0,>=2.0.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from flake8<3.4.1a,>=2.6.0->datadotworld[pandas])\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from flake8<3.4.1a,>=2.6.0->datadotworld[pandas])\n",
      "Requirement already satisfied: pyflakes<1.6.0,>=1.5.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from flake8<3.4.1a,>=2.6.0->datadotworld[pandas])\n",
      "Requirement already satisfied: pytz>=2011k in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from pandas<1.0a; extra == \"pandas\"->datadotworld[pandas])\n",
      "Requirement already satisfied: numpy>=1.7.0 in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from pandas<1.0a; extra == \"pandas\"->datadotworld[pandas])\n",
      "Requirement already satisfied: jdcal in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from openpyxl<3.0,>=2.4->tabulator<=1.4.1->datadotworld[pandas])\n",
      "Requirement already satisfied: et_xmlfile in /Users/jameswilfong/anaconda/lib/python3.6/site-packages (from openpyxl<3.0,>=2.4->tabulator<=1.4.1->datadotworld[pandas])\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<a id='notebook-config'></a>\n",
       "## Notebook Config\n",
       "* python-dotenv\n",
       "* datadotworld"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env\n",
    "\n",
    "cell_log.clear()\n",
    "cell_log.collect(\"<a id='notebook-config'></a>\")\n",
    "cell_log.collect(\"## Notebook Config\")\n",
    "# ------------ environment variable magic\n",
    "\n",
    "# Install a pip packages in the current Jupyter kernel\n",
    "# ------------ Python-dotenv\n",
    "cell_log.collect(\"* python-dotenv\")\n",
    "import sys\n",
    "!{sys.executable} -m pip install python-dotenv\n",
    "# ------------ data.world API \n",
    "cell_log.collect(\"* datadotworld\")\n",
    "!{sys.executable} -m pip install datadotworld[pandas]\n",
    "\n",
    "Markdown('''{}'''.format(cell_log.getMarkdown()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "## Prepare Data\n",
    "* download github repo with data\n",
    "* put new file in raw-data/\n",
    "* make copy of this jupyter notebook \n",
    "* configure to transform raw-data/ into clean-data/\n",
    "* put clean data into clean/ folder\n",
    "* push final changes to github\n",
    "## Load Data\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wrangling_steps'></a>\n",
    "# Data Wrangling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wrangle-process'></a>\n",
    "## Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODE:  prod\n",
      "LOCAL_RAW_FOLDER:  /Users/jameswilfong/Documents/Github/Wilfongjt/source-data/raw-data/\n",
      "LOCAL_CLEAN_FOLDER:  /Users/jameswilfong/Documents/Github/Wilfongjt/source-data/clean-data/\n"
     ]
    }
   ],
   "source": [
    "MODE='prod' # dev, prod\n",
    "LOCAL_RAW_FOLDER = os.getcwd().replace('notebook','raw-data') + '/'\n",
    "LOCAL_CLEAN_FOLDER = os.getcwd().replace('notebook','clean-data') + '/'\n",
    "print('MODE: ', MODE)\n",
    "print('LOCAL_RAW_FOLDER: ', LOCAL_RAW_FOLDER)\n",
    "print('LOCAL_CLEAN_FOLDER: ', LOCAL_CLEAN_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSourceData(tblDef):\n",
    "    return pd.read_csv(tblDef[\"local_raw\"])\n",
    "\n",
    "def deprecated_getEvaluation(df_source):\n",
    "    #\n",
    "    limit = 0\n",
    "    flds = df_source.columns.tolist()\n",
    "    dups =   {} # {fldname: count of duplicates, ...}\n",
    "    values = {} # {fldname:[list of values], ...}\n",
    "    blanks = {} # {fldname: count of blanks}\n",
    "    minmax = {} # {fldname: {min: value, max: value}\n",
    "    evaluation = {\"duplicates\": dups, \"blanks\": blanks, \"minmax\": minmax}\n",
    "    for fld in flds:\n",
    "        if fld not in dups:\n",
    "            dups[fld] = 0\n",
    "        if fld not in values:\n",
    "            values[fld] = []\n",
    "        if fld not in blanks:\n",
    "            blanks[fld]=0\n",
    "        if fld not in minmax:\n",
    "            minmax[fld]={}\n",
    "            \n",
    "    cnt = 0\n",
    "    # print(flds)\n",
    "    \n",
    "    for row in df_source.values: # loop drains\n",
    "        cnt += 1\n",
    "        # print(row)\n",
    "        for fld in flds:  # loop field names\n",
    "\n",
    "            ifld = flds.index(fld)\n",
    "            if row[ifld] in values[fld]:\n",
    "                dups[fld] += 1\n",
    "            else:\n",
    "                values[fld].append(row[ifld])\n",
    "            \n",
    "            if row[ifld] == ' ' or row[ifld] == None:\n",
    "                blanks[fld] += 1\n",
    "                \n",
    "            if 'min' in minmax[fld]:\n",
    "                if row[ifld] < minmax[fld]['min']:\n",
    "                    minmax[fld]['min'] = row[ifld]\n",
    "            else:\n",
    "                minmax[fld]['min'] = row[ifld]\n",
    "                \n",
    "            if 'max' not in minmax[fld]:  \n",
    "                minmax[fld]['max'] = row[ifld]\n",
    "            \n",
    "            if row[ifld] > minmax[fld]['max']:\n",
    "                minmax[fld]['max'] = row[ifld]\n",
    "            \n",
    "                \n",
    "                \n",
    "        if limit > 0 and limit < cnt:\n",
    "            break\n",
    "            \n",
    "    return evaluation\n",
    "        \n",
    "def getTableDef(table_name, ext='csv'):\n",
    "    return { \"owner_id\": DW_USER, \n",
    "             \"title\": table_name, \n",
    "             \"gh_url\": GH_URL + table_name, \n",
    "             \"visibility\": \"OPEN\", \n",
    "             \"license\": \"Public Domain\",\n",
    "             \"files\": {table_name + '.' + 'csv': {\"url\": GH_URL + table_name + '.' + ext}},\n",
    "             \"dw_url\": DW_DB_URL + table_name + '.' + ext, \n",
    "             \"local_raw\": LOCAL_RAW_FOLDER + table_name + '.' + ext,\n",
    "             \"local_clean\": LOCAL_CLEAN_FOLDER + table_name + '.' + ext\n",
    "           }\n",
    "\n",
    "\n",
    "def loadDataWorld(tbl_def):\n",
    "    '''\n",
    "        Takes a csv file and imports it into dataworld\n",
    "        tbl_def is { \"owner_id\": DW_USER, \n",
    "                     \"title\": table_name, \n",
    "                     \"gh_url\": GH_URL + table_name, \n",
    "                     \"visibility\": \"OPEN\", \n",
    "                     \"license\": \"Public Domain\",\n",
    "                     \"files\": {table_name + '.csv': {\"url\": GH_URL + table_name + '.csv'}},\n",
    "                     \"dw_url\": DW_DB_URL + table_name + '.csv' \n",
    "                    }\n",
    "                    \n",
    "    '''\n",
    "    # api_client.create_dataset(\n",
    "    dw.api_client().create_dataset(    \n",
    "        owner_id=tbl_def[\"owner_id\"], \n",
    "        title=tbl_def[\"title\"], \n",
    "        visibility=tbl_def[\"visibility\"],\n",
    "        license=tbl_def['license'],\n",
    "        files=tbl_def[\"files\"]\n",
    "    )\n",
    "    \n",
    "    \n",
    "def deleteDataWorld(tbl_def):\n",
    "    '''\n",
    "    Removes table from data.world\n",
    "    tbl_def is { \"owner_id\": DW_USER, \n",
    "                     \"title\": table_name, \n",
    "                     \"gh_url\": GH_URL + table_name, \n",
    "                     \"visibility\": \"OPEN\", \n",
    "                     \"license\": \"Public Domain\",\n",
    "                     \"files\": {table_name + '.csv': {\"url\": GH_URL + table_name + '.csv'}},\n",
    "                     \"dw_url\": DW_DB_URL + table_name + '.csv' \n",
    "                    }\n",
    "    '''\n",
    "    print(\"user/dataset: \" + tbl_def[\"owner_id\"] + \"/\" + tbl_def[\"title\"])\n",
    "    return dw.api_client().delete_dataset(       \n",
    "        tbl_def[\"owner_id\"] + \"/\" + tbl_def[\"dw_url\"]\n",
    "    )\n",
    "        \n",
    "    \n",
    "# def renameColumns(df,):    \n",
    "#    df = df.rename(columns=clean_column_names)    \n",
    "\n",
    "def setDuplicates(default_value, dup_list):\n",
    "    return lambda default_value : a * n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "------------- configure source csv\n",
    "'''\n",
    "table_name = 'gr_drains'\n",
    "repo_branch = 'refresh-data'\n",
    "'''\n",
    "------------- configure source csv\n",
    "'''\n",
    "tables = [\n",
    "    getTableDef(table_name)\n",
    "]\n",
    "'''\n",
    "------------- configure outliers\n",
    "'''\n",
    "_outliers = {\n",
    "  'outliers': [\n",
    "    {'column':'dr_facility_id',\n",
    "     'range':(1, 50000000),\n",
    "     'reason':'ignore {} outliers (1 <= dr_facility_id or => 50000000).',\n",
    "     'count': 0\n",
    "    }, \n",
    "    {'column':'dr_lon',\n",
    "     'range':(-90.0, -80.0),\n",
    "     'reason':'Remove {} observations too far west or east.',\n",
    "     'count': 0\n",
    "    },  \n",
    "    {'column':'dr_lat',\n",
    "     'range':(40.0, 50.0),\n",
    "     'reason':'Remove {} observations too far north or south.',\n",
    "     'count': 0\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'owner_id': 'wilfongjt', 'title': 'gr_drains', 'gh_url': 'https://raw.githubusercontent.com/Wilfongjt/source-data/refresh-data/clean-data/gr_drains', 'visibility': 'OPEN', 'license': 'Public Domain', 'files': {'gr_drains.csv': {'url': 'https://raw.githubusercontent.com/Wilfongjt/source-data/refresh-data/clean-data/gr_drains.csv'}}, 'dw_url': 'https://api.data.world/v0/datasets/wilfongjt/gr_drains.csv', 'local_raw': '/Users/jameswilfong/Documents/Github/Wilfongjt/source-data/raw-data/gr_drains.csv', 'local_clean': '/Users/jameswilfong/Documents/Github/Wilfongjt/source-data/clean-data/gr_drains.csv'}\n",
      "* clean_column_names: 0.0057489871978759766 sec\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 40189 entries, 0 to 40197\n",
      "Data columns (total 9 columns):\n",
      "dr_subtype         40189 non-null float64\n",
      "dr_jurisdiction    40189 non-null object\n",
      "dr_owner           40189 non-null object\n",
      "soure__id          40189 non-null object\n",
      "dr_local_id        40189 non-null object\n",
      "dr_facility_id     40189 non-null int64\n",
      "dr_subwatershed    40189 non-null object\n",
      "dr_lon             40189 non-null float64\n",
      "dr_lat             40189 non-null float64\n",
      "dtypes: float64(3), int64(1), object(5)\n",
      "memory usage: 3.1+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 40189 entries, 0 to 40197\n",
      "Data columns (total 9 columns):\n",
      "dr_subtype         40189 non-null float64\n",
      "dr_jurisdiction    40189 non-null object\n",
      "dr_owner           40189 non-null object\n",
      "dr_local_id        40189 non-null object\n",
      "dr_facility_id     40189 non-null int64\n",
      "dr_subwatershed    40189 non-null object\n",
      "dr_lon             40189 non-null float64\n",
      "dr_lat             40189 non-null float64\n",
      "dr_sync_id         40189 non-null object\n",
      "dtypes: float64(3), int64(1), object(5)\n",
      "memory usage: 3.1+ MB\n",
      "* remove_obvious_outliers: 0.009011030197143555 sec\n",
      "user/dataset: wilfongjt/gr_drains\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid dataset key. Key must include user and dataset names, separated by (i.e. user/dataset).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-88bdab139727>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mcell_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'* load data into data.world'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mdeleteDataWorld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d4e3553140a5>\u001b[0m in \u001b[0;36mdeleteDataWorld\u001b[0;34m(tbl_def)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user/dataset: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtbl_def\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"owner_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtbl_def\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     return dw.api_client().delete_dataset(       \n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mtbl_def\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"owner_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtbl_def\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/datadotworld/client/api.py\u001b[0m in \u001b[0;36mdelete_dataset\u001b[0;34m(self, dataset_key)\u001b[0m\n\u001b[1;32m    272\u001b[0m         ...     'username/dataset')  # doctest: +SKIP\n\u001b[1;32m    273\u001b[0m         \"\"\"\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mowner_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_dataset_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasets_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mowner_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/datadotworld/util.py\u001b[0m in \u001b[0;36mparse_dataset_key\u001b[0;34m(dataset_key)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_KEY_PATTERN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         raise ValueError('Invalid dataset key. Key must include user and '\n\u001b[0m\u001b[1;32m     53\u001b[0m                          'dataset names, separated by (i.e. user/dataset).')\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid dataset key. Key must include user and dataset names, separated by (i.e. user/dataset)."
     ]
    }
   ],
   "source": [
    "cell_log.clear()\n",
    "\n",
    "\n",
    "cell_log.collect(\"# CSV Process\")\n",
    "'''\n",
    "--------------------------------- input\n",
    "'''\n",
    "for tbl in tables:\n",
    "    cell_log.collect(\"* input:  {}\".format( tbl[\"local_raw\"]))\n",
    "\n",
    "'''\n",
    "--------------------------------- load data\n",
    "''' \n",
    "tbl = tables[0]\n",
    "# df_source = pd.read_csv(tbl[\"local_raw\"])\n",
    "print(tbl)\n",
    "df_source = getSourceData(tbl)\n",
    "cell_log.collect(\"* input: {} observations\".format(len(df_source)))\n",
    "cell_log.collect(\"* input: columns {}\".format(df_source.columns.values))\n",
    "\n",
    "'''\n",
    "--------------------------------- clean column names\n",
    "'''\n",
    "cell_log.collect('* format: Apply a style of lowercase and underscores to column names.')##############################\n",
    "df_source = clean.clean_column_names(df_source)\n",
    "\n",
    "'''\n",
    "--------------------------------- rename columns\n",
    "'''\n",
    "# df_source['lon'] = df_source['trk_crnt_x_cord']\n",
    "# df_source['lat'] = df_source['trk_crnt_y_cord']\n",
    "df_source = df_source.rename(columns={\n",
    "    \"subtype\": \"dr_subtype\",\n",
    "    \"drain__owner\": \"dr_owner\",\n",
    "    \"local__id\": \"dr_local_id\",\n",
    "    \"facilityid\": \"dr_facility_id\",\n",
    "    \"drain__jurisdiction\": \"dr_jurisdiction\",\n",
    "    \"subwatershed\": \"dr_subwatershed\",\n",
    "    \"point__x\":\"dr_lon\", \n",
    "    \"point__y\":\"dr_lat\"})\n",
    "\n",
    "# print('info: ',df_source.info())\n",
    "'''\n",
    "--------------------------------- change empty values\n",
    "'''\n",
    "# change '', ' ', None, and NaN to -1\n",
    "\n",
    "# def_facility_id = _outliers['outliers'][0]['range'][0] - 1\n",
    "\n",
    "# def_facility_id_dup =  def_facility_id - 1\n",
    "\n",
    "# cell_log.collect(\"* clean: mark bad dr_facility_id values ('',' ',None,NaN') with {}\".format(def_facility_id))\n",
    "\n",
    "## ------------------------------ DROP empty Facility id\n",
    "# mark all empties with same value\n",
    "df_source['dr_facility_id'] = df_source['dr_facility_id'].apply(lambda x:  np.nan if x != x or x == '' or x == ' ' or x == None else x)\n",
    "scnt = len(df_source)\n",
    "df_source = df_source.dropna(subset=['dr_facility_id', 'soure__id','dr_lon', 'dr_lat'])\n",
    "ecnt = len(df_source)\n",
    "cell_log.collect(\"* clean: dropped {} observations with empty dr_facility_id, soure___id, dr_lon, or dr_lat\".format(scnt - ecnt))\n",
    "\n",
    "\n",
    "'''\n",
    "--------------------------------- change column types\n",
    "'''\n",
    "cell_log.collect('* format: convert dr_facility_id column to int64')\n",
    "df_source['dr_facility_id'] = df_source['dr_facility_id'].astype('int64')\n",
    "\n",
    "'''\n",
    "--------------------------------- remove numbers from df_source_id\n",
    "'''\n",
    "df_source.info()\n",
    "# df_source['dr_source_id'] = df_source['dr_source_id'].apply(lambda x: x.split('_')[0] if '_' in x else x ) \n",
    "# df_source['dr_source_id'] = df_source['dr_source_id'].apply(lambda x: x if '_' in x else x )\n",
    "df_source['soure__id'] = df_source['soure__id'].apply(lambda x: x.split('_')[0] + '_' if isinstance(x, str) else 'XXX_') \n",
    "\n",
    "'''\n",
    "--------------------------------- create a sync id\n",
    "'''\n",
    "df_source['dr_sync_id'] = df_source['soure__id'] + df_source['dr_facility_id'].astype(str)\n",
    "\n",
    "'''\n",
    "--------------------------------- drop soure__id\n",
    "'''\n",
    "df_source = df_source.drop(['soure__id'], axis=1)\n",
    "\n",
    "'''\n",
    "--------------------------------- combine source_id and facility_id\n",
    "'''\n",
    "# df_source['dr_sync_id'] = df_source['dr_source_id'] + df_source['dr_facility_id'].astype(str)\n",
    "\n",
    "df_source.info()\n",
    "\n",
    "'''\n",
    "--------------------------------- outliers\n",
    "'''\n",
    "df_source = clean.remove_obvious_outliers(_outliers, df_source)\n",
    "# cell_log.collect('# Outliers')\n",
    "for r in _outliers['outliers']:##############################\n",
    "    cell_log.collect('* outlier: {}'.format(r['reason']))\n",
    "\n",
    "'''\n",
    "--------------------------------- Drop DUPLICATES\n",
    "'''\n",
    "scnt = len(df_source)\n",
    "df_source = df_source.drop_duplicates('dr_facility_id',keep=False)\n",
    "ecnt = len(df_source)\n",
    "cell_log.collect('* duplicates: dropped {} duplicate facility ids'.format(scnt - ecnt))\n",
    "\n",
    "\n",
    "'''\n",
    "--------------------------------- Final \n",
    "'''\n",
    "\n",
    "'''\n",
    "--------------------------------- save csv \n",
    "'''\n",
    "# cell_log.collect('# Output')\n",
    "# assume new file and remove old one\n",
    "if os.path.isfile(tbl[\"local_clean\"]):\n",
    "    os.remove(tbl['local_clean'])\n",
    "    cell_log.collect('* system: remove {} '.format(tbl['local_clean']))\n",
    "\n",
    "cell_log.collect(\"* inter-output: columns {}\".format(df_source.columns.values))\n",
    "cell_log.collect('* inter-output: {} obs to {}'.format(len(df_source) , tbl[\"local_clean\"]))\n",
    "\n",
    "\n",
    "df_source.to_csv(tbl[\"local_clean\"], index=False)\n",
    "\n",
    "\n",
    "if MODE == 'dev':\n",
    "    print('info: ',df_source.info())\n",
    "    print('head: ',df_source.head())  \n",
    "    print('outliers: ', _outliers)\n",
    "    \n",
    "    \n",
    "if MODE == 'prod':    \n",
    "    '''\n",
    "    run extra git commands\n",
    "    run import to data.word\n",
    "    '''\n",
    "    '''\n",
    "    --------------------------------- GIT Process \n",
    "    '''\n",
    "    cell_log.collect('')\n",
    "    cell_log.collect('# GIT Process')\n",
    "    '''\n",
    "    --------------------------------- input\n",
    "    '''\n",
    "    cell_log.collect('* input: ' + tbl[\"local_clean\"])\n",
    "    '''\n",
    "    --------------------------------- git add\n",
    "    '''\n",
    " \n",
    "    cell_log.collect('* git add raw-data/ -A')\n",
    "    output = subprocess.check_output([\"git\", \"add\", \"../raw-data\" ,\"-A\"])\n",
    "    cell_log.collect('* git add clean-data/ -A' )\n",
    "    output = subprocess.check_output([\"git\", \"add\", \"../clean-data\" ,\"-A\"])\n",
    "    cell_log.collect('* git add notebook/ -A' )\n",
    "    output = subprocess.check_output([\"git\", \"add\", \"../notebook\"])\n",
    "    cell_log.collect('* git add ../README.md -A' )\n",
    "    output = subprocess.check_output([\"git\", \"add\", \"../README.md\"])\n",
    "    '''\n",
    "    --------------------------------- git commit\n",
    "    '''\n",
    "    # cell_log.collect('* XXXXXXX git commit -m \"update raw-data {}\"'.format(tbl[\"local_raw\"]) )\n",
    "    # cell_log.collect('* XXXXXXX git commit -m \"update clean-data {}\"'.format(\"local_clean\") )\n",
    "    cell_log.collect('* git commit -m \"update raw-data, clean-data, and notebook files \"' )\n",
    "\n",
    "    # output = subprocess.check_output([\"git\", \"commit\", \"-m\", \"'update raw-data, clean-data, and notebook files'\"])\n",
    "\n",
    "    try:\n",
    "        output = subprocess.check_output([\"git\", \"commit\", \"-m\", \"'update raw-data, clean-data, and notebook files'\"])\n",
    "    except subprocess.CalledProcessError as error:\n",
    "        print(error)\n",
    "    except:\n",
    "        cell_log.collect('* unknown error' )\n",
    "    '''\n",
    "    --------------------------------- git push\n",
    "    '''\n",
    "    cell_log.collect('* git push origin ' + repo_branch)\n",
    "    output = subprocess.check_output([\"git\", \"push\", \"origin\", repo_branch])\n",
    "\n",
    "    '''\n",
    "    --------------------------------- Data World Process \n",
    "    '''\n",
    "    \n",
    "    cell_log.collect('')\n",
    "    cell_log.collect('# Data.World Process')\n",
    "    cell_log.collect('* input: {}'.format(tbl[\"gh_url\"] + \".csv\") )\n",
    "    cell_log.collect('* load data into data.world')\n",
    "    \n",
    "    deleteDataWorld(tbl)\n",
    "        \n",
    "    try:\n",
    "        print('try')\n",
    "        loadDataWorld(tbl)\n",
    "        cell_log.collect('* output: {}'.format(tbl[\"dw_url\"] ))\n",
    "   \n",
    " \n",
    "    except RestApiError as ex:\n",
    "        print('RestApiException ' + str(ex) )\n",
    "        cell_log.collect('* LOAD FAIL: {}'.format(str(ex)))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "Markdown('''{}'''.format(cell_log.getMarkdown()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
